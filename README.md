# ğŸ¯ Claude Code Toonify

**Reduce Claude API token usage by 60%+ using TOON format optimization**

An MCP (Model Context Protocol) server that transparently optimizes token usage in [Claude Code CLI](https://github.com/anthropics/claude-code) by converting structured data to TOON (Token-Oriented Object Notation) format.

## ğŸŒŸ Features

- **ğŸ¯ 60%+ Token Reduction**: Average 63.9% reduction for structured data
- **ğŸ’° Significant Cost Savings**: ~$1,380 per million API calls
- **âš¡ Fast**: <10ms overhead for typical payloads
- **ğŸ”„ Silent Fallback**: Never breaks your workflow
- **ğŸ“Š Built-in Metrics**: Track savings locally
- **ğŸ”Œ MCP Integration**: Works seamlessly with Claude Code
- **ğŸŒ Universal Compatibility**: Works with ANY LLM (GPT, Claude, Gemini, etc.)

## ğŸ“Š Performance

| Format | Before | After | Savings |
|--------|--------|-------|---------|
| JSON   | 247 bytes | 98 bytes | **60%** |
| CSV    | 180 bytes | 65 bytes | **64%** |
| YAML   | 215 bytes | 89 bytes | **59%** |

**Token reduction example:**
```
JSON (142 tokens):
{
  "products": [
    {"id": 101, "name": "Laptop Pro", "price": 1299},
    {"id": 102, "name": "Magic Mouse", "price": 79}
  ]
}

TOON (57 tokens, 60% reduction):
products[2]{id,name,price}:
  101,Laptop Pro,1299
  102,Magic Mouse,79
```

## ğŸš€ Installation

### 1. Install the package

```bash
npm install -g @ktseng/claude-code-toonify
```

### 2. Configure Claude Code

Add to your Claude Code settings (`~/.claude/settings.json`):

```json
{
  "mcpServers": {
    "toonify": {
      "command": "claude-code-toonify"
    }
  }
}
```

### 3. Verify installation

```bash
# Check if MCP server is registered
claude mcp list

# Test optimization
claude mcp call toonify optimize_content '{"content": "{\"test\": \"data\"}"}'
```

## ğŸ“– Usage

### Automatic Optimization

The MCP server automatically optimizes tool results when used with Claude Code:

```typescript
// In Claude Code, tool results are automatically intercepted
// Large JSON responses get optimized before sending to API
const fileContent = await tools.Read({ file_path: "data.json" });
// â†’ Automatically converted to TOON if > 50 tokens and structured
```

### Manual Optimization

Use the MCP tools directly:

```bash
# Optimize content
claude mcp call toonify optimize_content '{
  "content": "{\"products\": [{\"id\": 1, \"name\": \"Test\"}]}",
  "toolName": "Read"
}'

# Get statistics
claude mcp call toonify get_stats '{}'
```

## ğŸ”§ Configuration

Configure via environment variables or config file:

```bash
# Environment variables
export TOONIFY_ENABLED=true
export TOONIFY_MIN_TOKENS=50
export TOONIFY_MIN_SAVINGS=30
export TOONIFY_SKIP_TOOLS="Bash,Write"

# Or via ~/.claude/toonify-config.json
{
  "enabled": true,
  "minTokensThreshold": 50,
  "minSavingsThreshold": 30,
  "skipToolPatterns": ["Bash", "Write"]
}
```

## ğŸ“Š Metrics Dashboard

View your savings:

```bash
claude mcp call toonify get_stats '{}'
```

Output:
```
ğŸ“Š Token Optimization Stats
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total Requests: 1,234
Optimized: 856 (69.4%)

Tokens Before: 1,456,789
Tokens After: 567,234
Total Savings: 889,555 (61.1%)

ğŸ’° Cost Savings (at $3/1M input tokens):
   $2.67 saved
```

## ğŸŒ Universal Compatibility

### âœ… **Works with ANY LLM:**
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude 3.5, Claude Opus)
- Google (Gemini)
- Mistral, Llama, etc.

The TOON optimization reduces tokens for **all** LLM APIs.

### âœ… **Works with ANY MCP Client:**
- **Claude Code CLI** (what we designed for)
- **Claude Desktop App**
- **Custom MCP clients**
- **VSCode with MCP support**
- **Any tool implementing MCP protocol**

### ğŸ”§ **How It Works:**

**For Claude Code (automatic):**
```bash
# Configure once in settings.json
claude mcp call toonify optimize_content '{"content": "..."}'
```

**For Other MCP Clients:**
```javascript
// Any MCP client can call the same tools
await mcpClient.callTool("toonify", "optimize_content", {
  content: largeJsonData,
  toolName: "Read"
});
```

**For Direct LLM Usage (no MCP):**
```python
# Use Toonify directly in your code
from toonify import encode
import openai

data = {"products": [...]}
optimized_data = encode(data)  # 60% token reduction

# Works with any LLM
openai.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": f"Analyze: {optimized_data}"}]
)
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Claude Code    â”‚
â”‚   CLI            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Toonify MCP     â”‚
â”‚  Server          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ optimize_contentâ”‚ â† Compress structured data
â”‚ â€¢ get_stats      â”‚ â† View metrics
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TokenOptimizer   â”‚ â† TOON encoding
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MetricsCollector â”‚ â† Track savings
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Development

```bash
# Clone repository
git clone https://github.com/ktseng/claude-code-toonify.git
cd claude-code-toonify

# Install dependencies
npm install

# Build
npm run build

# Run tests
npm test

# Development mode (watch)
npm run dev
```

## ğŸ¯ When to Use

**âœ… Ideal for:**
- Large JSON responses from tools
- CSV/tabular data
- Structured API responses
- Database query results
- File contents with structured data

**âš ï¸ Not recommended for:**
- Plain text content
- Code files
- Highly irregular data structures
- Content < 50 tokens

## ğŸ“š Technical Details

### How It Works

1. **Interception**: MCP server intercepts tool results via `optimize_content` tool
2. **Detection**: Analyzes content to identify JSON, CSV, or YAML
3. **Optimization**: Converts to TOON format using `@scrapegraph/toonify`
4. **Validation**: Ensures savings > 30% threshold
5. **Fallback**: Returns original if optimization fails or savings too low

### Dependencies

- `@modelcontextprotocol/sdk` - MCP server framework
- `@scrapegraph/toonify` - TOON format encoder/decoder
- `tiktoken` - Token counting (Claude/GPT compatible)
- `yaml` - YAML parsing support

## ğŸ¤ Contributing

Contributions welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file

## ğŸ™ Credits

- [Toonify](https://github.com/ScrapeGraphAI/toonify) by ScrapeGraphAI team
- [Claude Code](https://github.com/anthropics/claude-code) by Anthropic
- Original inspiration: [awesome-llm-apps](https://github.com/Shubhamsaboo/awesome-llm-apps)

## ğŸ”— Links

- **NPM Package**: Coming soon
- **GitHub**: https://github.com/ktseng/claude-code-toonify
- **Issues**: https://github.com/ktseng/claude-code-toonify/issues
- **MCP Documentation**: https://code.claude.com/docs/mcp

---

**â­ If this tool saves you money on API costs, consider starring the repo!**
